#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue May 21 15:01:58 2019

@author: anatolechouard
"""

import math
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
from scipy.integrate import quad

'MH'
#N=10000 #nombre d'iterations
#x0=10 # point de depart
'Student'
#k=5 #parametre de Student (1 grosse variance, infini petite variance)
poids=1 #poids de Student

'Gaussiennes'
fact=5 #facteur entre la deuxieme loi gaussienne et la premiere
m=10 #moyenne de la grosse loi gaussienne
gauche=-5 #gauche de la zone interessante
droite=15 #droite de la zone interessante 
'Histogramme'
bins=100 #nombre de rectangles dans l'histogramme : bins grand => rectangles peu larges

'0. Test kernel estimator'

def kernel(n):
    xn=[]
    for i in range(n):
        xn+=[x0_invariant()]
    print(xn)
#    xn2 = np.random.randn(n)

    gkde=stats.gaussian_kde(xn) #nature de l'estimateur : gaussian kernel density estimator
#    gkde2=stats.gaussian_kde(xn2) #nature de l'estimateur : gaussian kernel density estimator
#    print('gkde.evaluate')
#    print(gkde.evaluate(0))
    def gkde_to_function(x):
        return gkde.evaluate(x)[0]
#    def gkde2_to_function(x):
#        return gkde2.evaluate(x)[0]
    def gkde3_to_function(x):
#        return abs(gkde_to_function(x)-gkde2_to_function(x))
        return abs(gkde_to_function(x)-distrib(x))
#        
#    X = np.linspace(-5,5,100) #espace sur lequel il estime
#    kdepdf = gkde.evaluate(X) #estime ici
##    kdepdf2 = gkde2.evaluate(X)
#    kdepdf3=abs(kdepdf - kdepdf2)
#    
#    
##    plt.figure()
##    out1=plt.hist(xn, bins, normed=1) #plt : dessine l'histogramme
##    out2=plt.hist(xn2, bins, normed=1) #plt : dessine l'histogramme
##    out2=np.histogram(abs(xn2-xn), bins, normed=1) #np : calcule mais NE DESSINE PAS l'histogramme
#    plt.plot(X, kdepdf, label='kde', color="g")
#    plt.plot(X, kdepdf2, label='kde2')
#    plt.plot(X, kdepdf3, label='kde3')
##    Y=np.asarray([gkde3_to_function(x) for x in X])
##    plt.plot(X,Y, label = 'difference')
#    
#    plt.title('Kernel Density Estimation')
#    plt.legend()
#    plt.show()
#    
#    I1=quad(gkde_to_function, gauche, droite)
#    I2=quad(gkde2_to_function, gauche, droite)
    I3=quad(gkde3_to_function, gauche, droite)
    return I3




def plot(tab):
    gkde=stats.gaussian_kde(tab) #nature de l'estimateur : gaussian kernel density estimator
    X = np.linspace(-10,10,100) #espace sur lequel il estime
    kdepdf = gkde.evaluate(X) #estime ici
    plt.plot(X, kdepdf, label='kde', color="g")
    
       

'1.1 Fonction de distribution'

def distrib(x):
    return (1/(1+fact))*((1/(math.sqrt(2*math.pi)))*math.e**((-x**2)/2) + fact*(1/(math.sqrt(2*math.pi)))*math.e**((-(x-m)**2)/2))


'1.2 Fonction de distribution corrigée' #nb fois x0 invariant puis kde
def distrib_corrigee(x,nb):
    TAB=[]
    for i in range(nb):
        x0=x0_invariant()
        TAB+=[x0]
    gkde=stats.gaussian_kde(TAB)
    return (gkde(x)[0])

'2.1 Plot de la fct de distribution'

def distribution():
    X=np.linspace(gauche,droite,bins)
    Y=np.asarray([distrib(x) for x in X])
    plt.plot(X,Y)
    
    plt.grid()
    plt.title('Fonction de distribution')
    plt.savefig('distribution.png',bbox_inches='tight')
    plt.show()
    plt.close()

'2.2 Plot de la fct de distribution corrigée'

def distribution_corrigee(nb):
    X=np.linspace(gauche,droite,bins)
    Y=np.asarray([distrib_corrigee(x,nb) for x in X])
    plt.plot(X,Y)
    
    plt.grid()
    plt.title('Fonction de distribution')
    plt.savefig('distribution.png',bbox_inches='tight')
    plt.show()
    plt.close()
    
'3.1 Algorithme de Metropolis-Hastings'

def MH(N,k,x0): #renvoie un tableau des abscisses x des points acceptes par Metropolis-Hastings
    x=x0
    f=distrib(x)

    samples=[x0]
    for i in range(N-1):
        prop=x+poids*np.random.standard_t(k) #Proposition avec loi de student de parametre k
        f=distrib(x)
        f2=distrib(prop)
        if (f>f2):
            u=np.random.rand()
            if u<(f2/f):
                f=f2
                x=prop
        else:
            f=f2
            x=prop
        samples.append(x) #eventuellement if i%s==0 append x pour enlever proprietes markov
    return samples

'3.2 MH2 : renvoie un tableau de tableaux de toutes les coord de x0, ..., xN'

def MH2(N,k,x0,nb): #renvoie un tableau des abscisses x des points acceptes par Metropolis-Hastings
    T=[]
    for i in range(nb):
        print(i)
        T+=[MH(N,k,x0)]
    TABLEAU=np.transpose(T)
    return TABLEAU

'''TABLEAU : taille N*nb

    TABLEAU[0] represente les nb x0
    TABLEAU[1] represente les nb x1
    ...
    TABLEAU[-1] represente les nb xN-1

    TABLEAU [0][nb-1] represente le x0 du nb-ieme MH
'''

def MH2_invariant(N,k,nb): #commence sous pi invariante
    T=[]
    for i in range(nb):
        print(i)
        T+=[MH(N,k,x0_invariant())]
    TABLEAU=np.transpose(T)
    return TABLEAU

'4. Erreur integrale du estimator en norme TV'

def erreur(N,k,x0,nb):
    TABLEAU=MH2(N,k,x0,nb) #points pour estimer la distribution
    gkde=stats.gaussian_kde(TABLEAU[-1])#nature de l'estimateur : gaussian kernel density estimator
    print(TABLEAU[-1])  
    def difference(x):
        return abs(gkde.evaluate(x)[0]-distrib(x))
    I = quad(difference, gauche, droite)
    return (I[0]/2)

def erreur_corrigee(N,k,x0,nb):
    TABLEAU=MH2(N,k,x0,nb) #points pour estimer la distribution
    gkde=stats.gaussian_kde(TABLEAU[-1])#nature de l'estimateur : gaussian kernel density estimator
    print(TABLEAU[-1])  
    def difference(x):
        return abs(gkde.evaluate(x)[0]-distrib_corrigee(x,nb))
    I = quad(difference, gauche, droite)
    return (I[0]/2)

def erreur2(tab):
    gkde=stats.gaussian_kde(tab)#nature de l'estimateur : gaussian kernel density estimator
    print(tab)  
    def difference(x):
        return abs(gkde.evaluate(x)[0]-distrib(x))
    I = quad(difference, gauche, droite)
    return (I[0]/2)

def erreur2_corrigee(tab,nb):
    gkde=stats.gaussian_kde(tab)#nature de l'estimateur : gaussian kernel density estimator
    print(tab)  
    def difference(x):
        return abs(gkde.evaluate(x)[0]-distrib_corrigee(x,nb))
    I = quad(difference, gauche, droite)
    return (I[0]/2)

'4.2 Plot integrale de estimator' #Pour verifier que erreur est bien calculee

def plot_estimator(N,k,x0,nb):
    X=np.linspace(gauche,droite,bins) #espace sur lequel on estime
    
    TABLEAU=MH2(N,k,x0,nb) #points pour estimer la distribution
    gkde=stats.gaussian_kde(TABLEAU[-1])#nature de l'estimateur : gaussian kernel density estimator
    kdepdf = gkde.evaluate(X) #estime ici sur l'espace X
    
    def difference(x):
        return abs(gkde.evaluate(x)[0]-distrib(x))
    
#    plt.figure()
    Y=np.asarray([distrib(x) for x in X]) #distribution de base
    Z=np.asarray([difference(x) for x in X]) #difference
    
    plt.plot(X,Y, label='distribution', color="b") #plot distribution de base
    plt.plot(X, kdepdf, '--', label='kde') #plot estimateur

    plt.plot(X,Z, '-.', label='difference') #plot difference
#    plt.plot(X, abs(kdepdf-Y), label='courbe que devrait etre difference') #plot la vraie difference
    
    plt.grid()
    plt.title('Kernel Density Estimation')
    plt.legend()
    plt.show()
    
    
def plot_estimator_corrigee(N,k,x0,nb):
    X=np.linspace(gauche,droite,bins) #espace sur lequel on estime
    
    TABLEAU=MH2(N,k,x0,nb) #points pour estimer la distribution
    TAB=[]
    for i in range(nb):
        x0=x0_invariant()
        TAB+=[x0]
    
    gkde=stats.gaussian_kde(TABLEAU[-1])#nature de l'estimateur : gaussian kernel density estimator
    kdepdf = gkde.evaluate(X) #estime ici sur l'espace X
    
    gkde2=stats.gaussian_kde(TAB)
    kdepdf2 = gkde2.evaluate(X)
    def difference(x):
        return abs(gkde.evaluate(x)[0]-gkde2.evaluate(x)[0])
    
#    plt.figure()
    Z=np.asarray([difference(x) for x in X]) #difference
    
    plt.plot(X, kdepdf2, label='distribution corrigee', color="b") #plot distribution de base
    plt.plot(X, kdepdf, '--', label='kde') #plot estimateur
    plt.plot(X,Z, '-.', label='difference') #plot difference
#    plt.plot(X, abs(kdepdf-Y), label='courbe que devrait etre difference') #plot la vraie difference
    
    plt.grid()
    plt.title('Kernel Density Estimation corrected version')
    plt.legend()
    plt.show()
    
def plot_estimator2(tab): #idem plot_estimator mais a faire avec TABLEAU(N,k,x0,nb)[-1]
    X=np.linspace(gauche,droite,bins) #espace sur lequel on estime
    gkde=stats.gaussian_kde(tab)#nature de l'estimateur : gaussian kernel density estimator
    kdepdf = gkde.evaluate(X) #estime ici sur l'espace X
    
    def difference(x):
        return abs(gkde.evaluate(x)[0]-distrib(x))
    
    plt.figure()
    Y=np.asarray([distrib(x) for x in X]) #distribution de base
    Z=np.asarray([difference(x) for x in X]) #difference
    
    plt.plot(X,Y, label='distribution', color="b") #plot distribution de base
    plt.plot(X, kdepdf, '--', label='kde') #plot estimateur
    plt.plot(X,Z, '-.', label='difference') #plot difference
    
    plt.grid()
    plt.title('Kernel Density Estimation')
    plt.legend()
#    plt.show()

def plot_estimator2_corrigee(tab,nb): #idem plot_estimator mais a faire avec TABLEAU(N,k,x0,nb)[-1]
    X=np.linspace(gauche,droite,bins) #espace sur lequel on estime
    gkde=stats.gaussian_kde(tab)#nature de l'estimateur : gaussian kernel density estimator
    kdepdf = gkde.evaluate(X) #estime ici sur l'espace X
    TAB=[]
    for i in range(nb):
        x0=x0_invariant()
        TAB+=[x0]
    gkde2=stats.gaussian_kde(TAB)
    kdepdf2 = gkde2.evaluate(X)
    
    def difference(x):
        return abs(gkde.evaluate(x)[0]-gkde2.evaluate(x)[0])
    
    
    plt.figure()
    Z=np.asarray([difference(x) for x in X]) #difference
    
    plt.plot(X,kdepdf2, label='distribution', color="b") #plot distribution de base
    plt.plot(X, kdepdf, '--', label='kde') #plot estimateur
    plt.plot(X,Z, '-.', label='difference') #plot difference
    
    plt.grid()
    plt.title('Kernel Density Estimation')
    plt.legend()
#    plt.show()
'5. Precision de Metropolis-Hastings avec histogrammes'

def precision(N,k,x0,nb): #evalue l'integrale entre la fct de distrib et MH, nb : nb de repetitions
    e=0
#    X=np.linspace(gauche,droite,bins)
#    Y=np.asarray([distrib(x) for x in X])
#    plt.plot(X,Y)
    for j in range(nb):
        estimation=0
        samples=MH(N,k,x0)
#        out=plt.hist(samples, bins, normed=1) #plt : dessine l'histogramme
        out=np.histogram(samples, bins, normed=1) #np : calcule mais NE DESSINE PAS l'histogramme
        depart=out[1][0] #abscisse de depart
        arrivee=out[1][-2] #abscisse d'arrivee
        pas=out[1][1]-depart        
        i=depart
        compt=0
        while (i<=arrivee):
            estimation+=abs(distrib(i)-out[0][compt])*pas
            i+=pas
            compt+=1
        e+=estimation
    return (e/nb)

def precision2(N,k,x0,nb): #evalue l'integrale entre la fct de distrib et MH, nb : nb de repetitions
    e=0
#    X=np.linspace(gauche,droite,bins)
#    Y=np.asarray([distrib(x) for x in X])
#    plt.plot(X,Y)
    for j in range(nb):
        estimation=0
        TABLEAU=MH2(N,k,x0,nb)
#        out=plt.hist(TABLEAU[-1], bins, normed=1) #plt : dessine l'histogramme
        out=np.histogram(TABLEAU[-1], bins, normed=1) #np : calcule mais NE DESSINE PAS l'histogramme
        depart=out[1][0] #abscisse de depart
        arrivee=out[1][-2] #abscisse d'arrivee
        pas=out[1][1]-depart        
        i=depart
        compt=0
        while (i<=arrivee):
            estimation+=abs(distrib(i)-out[0][compt])*pas
            i+=pas
            compt+=1
        e+=estimation
    return (e/nb)

'6.1 Graphe : Precision en fonction de k, N fixe AVEC HISTOGRAMMES'

def graphePrecision(N,x0,nb,nbk,kf,kl): #precision en fonction de k
#N : nb d'iterations
#x0 : point de depart de MH
#nb : nb de repetitions de MH
#nbk : nb de k differents que l'on teste sur le graphe
#kf : k first : premier k que l'on teste
#kl : k last : dernier k que l'on teste
    
    tabK=[] #parametre de Student en abscisse
    tabPrecision=[] # Precision en ordonnee
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        tabPrecision+=[precision(N,k,x0,nb)]
    plt.figure()
    plt.plot(tabK,tabPrecision)
    plt.grid()
    plt.title('Graphe Precision')
    plt.savefig('graphe_precision.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabPrecision

def graphePrecision2(N,x0,nb,nbk,kf,kl): #precision en fonction de k
#N : nb d'iterations
#x0 : point de depart de MH
#nb : nb de repetitions de MH
#nbk : nb de k differents que l'on teste sur le graphe
#kf : k first : premier k que l'on teste
#kl : k last : dernier k que l'on teste
    
    tabK=[] #parametre de Student en abscisse
    tabPrecision=[] # Precision en ordonnee
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        tabPrecision+=[precision2(N,k,x0,nb)]
    plt.figure()
    plt.plot(tabK,tabPrecision)
    plt.grid()
    plt.title('Graphe Precision')
    plt.savefig('graphe_precision.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabPrecision

'6.2.1 Graphe : Erreur en fonction de k, N fixe AVEC GKDE'

def graphe_erreur(N,x0,nb,nbk,kf,kl): #erreur en fonction de k
#N : nb d'iterations
#x0 : point de depart de MH
#nb : nb de repetitions de MH
#nbk : nb de k differents que l'on teste sur le graphe
#kf : k first : premier k que l'on teste
#kl : k last : dernier k que l'on teste
    
    tabK=[] #parametre de Student en abscisse
    tabErreur=[] # Precision en ordonnee
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        TABLEAU=MH2(N,k,x0,nb)
        tabErreur+=[erreur2(TABLEAU[-1])]
        plot_estimator2(TABLEAU[-1])
    plt.figure()
    plt.plot(tabK,tabErreur)
    plt.grid()
    plt.title('Graphe : erreur en fonction de k, N={}, nb={}'.format(N,nb))
    plt.savefig('graphe_erreur.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabErreur

'6.2.2 Graphe : Erreur corrigee en fonction de k, N fixe AVEC GKDE'

def graphe_erreur_corrigee(N,x0,nb,nbk,kf,kl): #erreur en fonction de k
#N : nb d'iterations
#x0 : point de depart de MH
#nb : nb de repetitions de MH
#nbk : nb de k differents que l'on teste sur le graphe
#kf : k first : premier k que l'on teste
#kl : k last : dernier k que l'on teste
    
    tabK=[] #parametre de Student en abscisse
    tabErreur=[] # Precision en ordonnee
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        TABLEAU=MH2(N,k,x0,nb)
        tabErreur+=[erreur2_corrigee(TABLEAU[-1],nb)]
        plot_estimator2_corrigee(TABLEAU[-1],nb)
    plt.figure()
    plt.plot(tabK,tabErreur)
    plt.grid()
    plt.title('Graphe : erreur corrigee en fonction de k, N={}, nb={}'.format(N,nb))
    plt.savefig('graphe_erreur.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabErreur

'7.1 Graphe : N en fonction de k, precision fixee : on cherche pout tout k le N tel que precision<eps'
#on raisonne par dichotomie pour trouver N
def grapheNk(x0,nb,nbk,kf,kl,eps):
    tabK=[] #parametre de Student en abscisse
    tabN=[] # N en ordonnee
    tabP=[]
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        c1=0 #compteur 1
        c2=0 #compteur 2
        N=1000 #N de depart
        Nfinal=N
        p=precision(N,k,x0,nb)
        pfinal=p
        while (c1<20):
            if (p>eps):
                N=math.floor(N*(10-(c1/2)))
            else:
                pfinal=p
                Nfinal=N
                c2+=1
                N=math.floor(N/(6-(c2/4)))
            p=precision(N,k,x0,nb)
            c1+=1
        tabP+=[pfinal]
        tabN+=[Nfinal]
    plt.figure()
    plt.plot(tabK,tabN)
    plt.grid()
    plt.title('Graphe N en fonction de k')
    plt.savefig('graphe_Nk.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabP

'7.2 Graphe7.1 AVEC GKDE et AVEC OPTIMISATION TEMPS : un seul MH'

def grapheNk2(x0,nb,nbk,kf,kl,eps):
    tabK=[] #parametre de Student en abscisse
    tabN=[] # N en ordonnee
    tabE=[]
    
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        
        c1=0
        c2=0
        N=1000
        Nfinal=N
        TABLEAU=MH2(1000,k,x0,nb)
        tab=TABLEAU[-1]
        e=erreur2(tab)
        efinal=e
        
        while (c1<20):
            if (e>eps):
                N=min(1000,math.floor(N*(2-(c1/20))))
                tab=TABLEAU[N-1]
            else:
                efinal=e
                Nfinal=N
                c2+=1
                print(c2)
                N=max(1,math.floor(N/(1.5-c2/40)))
                tab=TABLEAU[N-1]
                
            e=erreur2(tab)
            c1+=1
            print(c1)
        plot_estimator2(tab)
        tabE+=[efinal]
        tabN+=[Nfinal]
    plt.figure()
    plt.plot(tabK,tabN)
    plt.grid()
    plt.title('Graphe N en fonction de k')
    plt.savefig('graphe_Nk.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabE

def grapheNk2_corrigee(x0,nb,nbk,kf,kl,eps):
    tabK=[] #parametre de Student en abscisse
    tabN=[] # N en ordonnee
    tabE=[]
    
    for j in range(nbk):
        k=kf+((kl-kf)/nbk)*j
        print(k)
        tabK+=[k]
        
        c1=0
        c2=0
        N=1000
        Nfinal=N
        TABLEAU=MH2(1000,k,x0,nb)
        tab=TABLEAU[-1]
        e=erreur2_corrigee(tab,nb)
        efinal=e
        
        while (c1<20):
            if (e>eps):
                N=min(1000,math.floor(N*(2-(c1/20))))
                tab=TABLEAU[N-1]
            else:
                efinal=e
                Nfinal=N
                c2+=1
                print(c2)
                N=max(1,math.floor(N/(1.5-c2/40)))
                tab=TABLEAU[N-1]
                
            e=erreur2_corrigee(tab,nb)
            c1+=1
            print(c1)
        plot_estimator2_corrigee(tab,nb)
        tabE+=[efinal]
        tabN+=[Nfinal]
    plt.figure()
    plt.plot(tabK,tabN)
    plt.grid()
    plt.title('Graphe N en fonction de k')
    plt.savefig('graphe_Nk.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabE

'8. Graphe : erreur en fonction de N, k fixe' #tres lent

def graphe_erreur_N(k,x0,nb,nbN,Nf,Nl):
    tabN=[] # N en abscisse
    tabErreur=[] # erreur en ordonnee
    
    for j in range(nbN):
        N=int(Nf+((Nl-Nf)/nbN)*j)
        print(N)
        tabN+=[N]
        print(tabN)
        TABLEAU=MH2(N,k,x0,nb)
        tabErreur+=[erreur2(TABLEAU[-1])]
        plot_estimator2(TABLEAU[-1])
    plt.figure()
    plt.plot(tabN,tabErreur)
    plt.grid()
    plt.title('Graphe : erreur en fonction de N, k={}, nb={}'.format(k,nb))
    plt.savefig('graphe_erreur_N.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabErreur

def graphe_erreur_N_corrigee(k,x0,nb,nbN,Nf,Nl):
    tabN=[] # N en abscisse
    tabErreur=[] # erreur en ordonnee
    
    for j in range(nbN):
        N=int(Nf+((Nl-Nf)/nbN)*j)
        print(N)
        tabN+=[N]
        print(tabN)
        TABLEAU=MH2(N,k,x0,nb)
        tabErreur+=[erreur2_corrigee(TABLEAU[-1],nb)]
        plot_estimator2_corrigee(TABLEAU[-1],nb)
    plt.figure()
    plt.plot(tabN,tabErreur)
    plt.grid()
    plt.title('Graphe : erreur en fonction de N, k={}, nb={}'.format(k,nb))
    plt.savefig('graphe_erreur_N.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabErreur

'9. Variance asymptotique'

#1. x0 sous pi invariant
def x0_invariant():
    rand=np.random.rand()
    if (rand<1/(1+fact)):
        x0=np.random.normal(0, 1, 1)[0]
    else:
        x0=np.random.normal(10, 1, 1)[0]
    return x0

#2. fixer une fct f : f(x)=x**2 par exemple  
def f(x):
    return x**2

#3. calucler variance asymptotique
def variance(N,k,nb):
    TABLEAU=MH2_invariant(N,k,nb)
    print('TABLEAU')
    print(TABLEAU)
    tab_sommes=[]    
    for i in range(nb):
        s=0
        for j in range(N):
            s+=f(TABLEAU[j][i])
        tab_sommes+=[s/N]
    print(tab_sommes)
    return np.var(tab_sommes)

#4. Variance asymptotique en fonction de k
def graphe_variance_k(N,nb,nbk,kf,kl):
    tabk=[] #k en abscisse
    tabVariance=[] #variance en ordonnee
    
    for j in range(nbk):
        k=int(kf+((kl-kf)/nbk)*j)
        print(k)
        tabk+=[k]
        print(tabk)
        tabVariance+=[variance(N,k,nb)]
    plt.figure()
    plt.plot(tabk,tabVariance)
    plt.grid()
    plt.title('Graphe : Variance en fonction de k, N={}, nb={}'.format(N,nb))
    plt.savefig('graphe_variance_k.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabVariance

#5. ESJD   
def ESJD(N,k,nb):
    TABLEAU=MH2_invariant(N,k,nb)
    print(TABLEAU)
    ESJD=0
    for i in range(nb):
        s=0
        for j in range(N-1):
            s+=((TABLEAU[j+1][i]-TABLEAU[j][i])**2)
        ESJD+=s
    return (ESJD/(N*nb))

#6. graphe ESJD en fonction de k
    
def graphe_ESJD_k(N,nb,nbk,kf,kl):
    tabk=[] #k en abscisse
    tabESJD=[] #variance en ordonnee
    
    for j in range(nbk):
        k=int(kf+((kl-kf)/nbk)*j)
        print(k)
        tabk+=[k]
        print(tabk)
        tabESJD+=[ESJD(N,k,nb)]
    plt.figure()
    plt.plot(tabk,tabESJD)
    plt.grid()
    plt.title('Graphe : ESJD en fonction de k, N={}, nb={}'.format(N,nb))
    plt.savefig('graphe_ESJD_k.png',bbox_inches='tight')
    plt.show()
    plt.close()
    return tabESJD


'''A FAIRE

1. [DONE] Tester kde avec seulement une gaussienne banale pour voir à quelle N (ou T) on a erreur < 0.01 par ex
2. [DONE] Pour le rapport : mettre courbes en pointilles
3. [DONE] Faire nouveau graphe : erreur en fonction de N à k fixé
4. [DONE] VARIANCE ASYMPTOTIQUE
5. [DONE] FAIRE EN SORTE DE POUVOIR PLOTER LES COURBES MEME QUAND JE NE LE FAIS PAS POUR L'INSTANT (ex : quand je calcule une erreur : ploter le graphe associé)

6. Remplacer student par gaussienne pour faire k<1
7. peut etre rapprocher gaussiennes dans distrib
8. Ploter histogramme et kde en même temps pour voir si kde bon ou non

A FAIRE 2
1. kde à remplacer par mixture of gaussian estimator
=> j'aurai ainsi besoin de moins de nb car l'erreur va etre bien plus faible
2. ESJD et variance : ajouter f indicatrice de x>12 par exemple : dans une queue de distribution pour voir
si le graphe de la variance va dans l'autre sens
3. ESJD et variance : rajouter compteur pour voir le pourcentage de propositions qui ont été
acceptés/rejetés dans MH
'''
